---
title: "HarvardX; Data Science Capstone"
subtitle: "MovieLens Recommndation System"
author: "Tali Behar"
date: "December 2020"
output:
  pdf_document:
    number_sections: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center", out.width = '65%',
                      message=FALSE, warning=FALSE)
```
# Executive Summary

## Overview

Recommendation systems are systems that are designed to recommend things to the user based on many different factors.

The goal of a recommender system is to generate meaningful recommendations by learning the user preferences and predicting the rating a user would give to an item. Then project that learning on future users of the system as well, in a way that keeps improving over time.

Suggestions for books on Amazon, or movies on Netflix, are real-world examples of the operation of industry-strength recommender systems. 

Recommender systems are an important class of machine learning algorithms. 

## Background

MovieLens dataset was collected over a long period of time by GroupLens Research.

Here we are using a version that contains about 10 million rows. 

Each row represents a movie rating given by a user.

We will use the MovieLens dataset in order to implement a movie recommendation system that will help us predict which movies a user is more likely to watch, based on his history ratings.

The dataset contains the following column for each row:

* UserId (contains ~70k different users)

* MovieId (contains ~11k different movies)

* Rating (varies between 0.5-5)

* Timestamp (when the rating was give)

* Title + Release year

* Genre (each movie can be categorized with multiple genres)

On average, each user rates 103 movies. This means that each user rated only a subset of the movies based on his preferences, and not all of them.

## Goal

The goal of the project is to analyze movie i characteristics that can affect the rating of a given user u, and by that predict the rating that the user u will rate that movie.

## Variable Selection

1.	The outcome of the movie we are predicting; **Y = rating**

2.	The characteristics we will use for the prediction:

  * **Mean**: average of all rated movies in the system, regardless of the user
  
  * **User**: users are different with their personal preferences, as well as in their engagement and excitement level. We’ll check the user specific effect on the mean.
  
  * **Movie**: some movies tend to get rated higher on average than others (regardless of the user).we’ll check the movie specific effect on the mean. 
  
  * **Movie age**: older movies tend to be rated higher than new movies. We’ll check how the age of the movie affect the user ratings. 
  
  * **Time effect**: newer movies have less ratings than older movies. We’ll use that by adding the time effect to the average rating for movie i and the specific effect of user u.
  
  * **Genre**: different genres get rated differently, both in the score and quantity of ratings. We’ll add the genres effect and the time effect to the average rating for movie i and the specific effect of user u.
    
## Method 

  * Divide the MovieLens dataset randomly to train set (“edx”) and validation set (the final hold-out test set)
  
  * Divide “edx”  randomly to train and test set in order to train and examine the different models.
  
  * Penalize outliers - cross validation in order to find penalty term- diving the data once again in order to prevent overfiting

## Model Estimation

Evaluate model performance based on RMSE (Root Mean Square Error).

RMSE is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are. RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.

The desired RMSE goal we get is **RMSE < 0.8649**

## Model Results 

The chosen model was test on the validation set and resulted in **RMSE = 0.8632**

## Conclusion

The most accurate model is the one that took into account all the features: 

Reg. Movie + User Effect + Time Effect + Genres Effect 

The test validation showed even better results than the results we obtained while training the model, which means that the algorithm can act standalone in the future.

# Data Preperation

## Installing Required Packeges

```{r required packeges, eval=FALSE, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(DescTools)) install.packages("DescTools", repos = "http://cran.us.r-project.org")
if(!require(colorspace)) install.packages("colorspace", repos = "http://cran.us.r-project.org")
if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
```

```{r install required packeges, echo=TRUE}
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(scales)
library(gridExtra)
library(knitr)
library(DescTools) 
library(colorspace)
library(cowplot)
library(formattable)
```

## Loading MovieLens Dataset

```{r}
#Since we don't want to download and process the dataset every time we can run this report as it takes a significant time, we run it once in the movielens R script, and export the environment to an RData file, that already contains the edx, validation and edx_year_sanitized. 
# The file can be downloaded once from OneDrive and placed in the same folder as this Rmd file: https://1drv.ms/u/s!Atq3A_0qvJi1h9A_Rdbn5z5PssjzGw?e=hibyvj
# To use this optimization, simply uncomment this line, and comment out the following download code chunks.

#load("movielens_dataset.RData")
####

```

```{r Loading Movielens Dataset, echo=TRUE}
# Disable this code chunk if using the optimized process with an RData file

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

### 3.Create Edx set (training set) and Validation Sets (final hold-out test set)

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                          title = as.character(title),
                                         genres = as.character(genres))

# if using R 4.0 or later:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
#                                            title = as.character(title),
#                                            genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

## Get A Glimpse Of Edx Data   
```{r glimpse Of edx, echo=FALSE}
glimpse(edx)
```
'edx' data set contains 6 columns; 

1. userId - Unique ID for the user

2. movieId - Unique ID for the movie

3. rating - A rating between 0.5 and 5 for the movie

4. timestamp - Date and time the rating was given

5. title - Movie title and year the movie was released.

6. genres - Genres associated with the movie

And 9 million rows. Each row provides a given rating to one movie by specific user. 

## Sanitizing the data - timestamp column

The date-time in the 'edx' timestamp column is a point on the timeline, stored as the number of seconds since 1970-01-01 00:00:00 UTC.

In order to be able to use the time information and for better convenience, I converted the timestamp into rate date, added rate year column, extracted the release year from the movie title and deleted the release year from movie title
```{r sanitizing the data, echo=TRUE}
# Disable this code chunk if using the optimized process with an RData file

edx_year_sanitized <- edx %>% 
  mutate(rate_year = year(as_datetime(timestamp)),
          rate_date = date(as_datetime(timestamp)),
          release_year =as.numeric(str_extract(title, "(?<=\\()(\\d{4})(?=\\))")),
          title= str_remove(as.character(title),"(\\(\\d{4}\\))")) %>% 
  select(-timestamp)
```

Get a glimpse of the sanitized data 
```{r glimpse of the sanitized data, echo=FALSE}
glimpse(edx_year_sanitized)
```

## The Dataset Overview
```{r summarized edx_year_sanitized data, echo=FALSE}
edx_year_sanitized %>% 
  summarize("Number of users" = n_distinct(userId),
            "Number of movies" = n_distinct(movieId), 
            "Number of ratings (M)" = nrow(edx)/1000000,
            "Number of 'missing' ratings (M)"=
              ((n_distinct(userId)*n_distinct(movieId))-nrow(edx))/1000000) %>%
  knitr::kable()
```
'edx_year_sanitized' contains about 70k different users that provided ratings and about 11k different rated movies.

The following matrix contain random sample of 120 movies and 120 users. 

If we were to look at the entire matrix, the empty cells (about 737 million cells in the data set overall), represent the unrated movies by all users. 
Users rated only the selected movies by their personal preferences, therefore we have "only" 9 million ratings. 

Each of the colored cells represent one rating and the color varies according to its score. 
```{r  figure 1 preview matrix, echo=FALSE}
 
users <- sample(unique(edx_year_sanitized$userId), 120)
edx_year_sanitized %>% 
  filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>% 
  select(sample(ncol(.), 120)) %>% 
  as.matrix() %>% 
  t(.) %>%
  image(1:120, 1:120,. , xlab="Movies", ylab="Users")
abline(h=0:120+0.5, v=0:120+0.5, col = "whitesmoke")
title("User / Movie Rating Combination")
```

**Evaluate the movie - Rating Score**
```{r rating score summarize, echo=FALSE}
# rating score - mean, median, mode
edx_year_sanitized %>% 
  summarize(mean=mean(rating),
            median=median(rating),
            mode=Mode(rating))%>% 
  knitr::kable()
```
The average rate score is 3.512 stars while both median and mode rate score is 4 stars (distribution is negatively skewed).

About 2.5 milion ratings, which is 29% of the total ratings, are 4 stars and only 41% of the ratings is 3 stars and less.

In general, whole star ratings are more common than half star ratings, and are 79.5 percent of the total ratings, as shown in the next plot. 
```{r figure 2 graph rating score distribution, echo=FALSE}

data.frame(edx_year_sanitized$rating, 
stars=ifelse(floor(edx_year_sanitized$rating)==edx_year_sanitized$rating,
             "whole_star","half_star"))%>% 
group_by(edx_year_sanitized.rating, stars) %>% 
summarize(count=n()) %>% 
ggplot(aes(x=edx_year_sanitized.rating,y=(count/sum(count)),fill = stars))+
geom_bar(stat='identity') +
scale_x_continuous(breaks=seq(0.5, 5, by= 0.5)) +
scale_y_continuous(labels=percent)+
scale_fill_manual(values = c("half_star"="lightskyblue", "whole_star"="blue")) +
geom_vline(xintercept=mean(edx_year_sanitized$rating) ,color="black", linetype="dashed", size=0.5)+
geom_vline(xintercept=median(edx_year_sanitized$rating) ,color="red", linetype="dashed", size=0.5)+
labs(x="Stars Rating", y="Ratings Percentage") +
ggtitle("Stars Ratings Percentage")
``` 
    * Top 15 blockbuster movies
```{r figure 3 graph blockbuster movies, echo=FALSE}
edx_year_sanitized %>% 
  group_by(title) %>%
  summarize(count_k=n()/1000, avg=mean(rating)) %>% # count in thousands
  top_n(15,count_k)%>%
  ggplot(aes(avg,count_k,lable=title)) +
  geom_point()+
  geom_text(aes(label=title), size=3,  hjust=0,vjust=0)+
  xlim(3,5)+
  ggtitle("Top 15 blockbuster movies")+
  labs(x="Rating Score", y="Total Number Of Ratings(Thousands)")
```
The plot above demonstrates the top 15 blockbuster movies with overall number of ratings higher than 22k, most of them are rated above the average score.

The top leader is Pulp Fiction, directed by Quentin Tarantino, and was released on 1994.


  **The challenge of recommendation systems**

Since our goal is to predict the future ratings, we can treat this as a machine learning challenge and "fill" the empty cells from the above matrix.

From the user's prespective, we'll want to fit a rating which is as close as possible to the actual rating the user would have rated the movie, and use that for future users with similiar preferences outside of our control.

From the movie's prespective, we'll want to predict if the movie could become a blockbuster, or less than that.

We'll use the following features: UserId, MovieId, Age of the movie at rating, time (release year) and genre.

# Analysis And Modeling

## Create additional partition of training and test set

In this step, we'll create additional training and test sets, in order to evaluate the accuracy of the models we build and to prevent overfitting of the RMSE.

Once we hit **RMSE < 0.8649**, we'll use the final holdout test set (validation) and determine the final model RMSE.
```{r additional partition}
set.seed(755, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(755)`

# randomly splitting edx data set into 80% training set and 20% testing set 
test_index <- createDataPartition(y = edx_year_sanitized$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_edx <- edx_year_sanitized %>% slice(-test_index)
temp <- edx_year_sanitized %>% slice(test_index)

# making sure that the test set includes users and movies that appear in the training set.  
test_edx <- temp %>% 
  semi_join(train_edx, by = "movieId") %>%
  semi_join(train_edx, by = "userId")
removed <- anti_join(temp,test_edx)
train_edx <- rbind(train_edx, removed)
rm(temp, removed)
```

## RMSE function

  * **MSE Function**
  
      The MSE measures the average of the squares of the errors that is, the average squared difference between the estimated values and the actual value. Therefore, is a measure of the quality of an estimator, it is always non-negative, and values closer to zero are better.

$$ MSE = {\frac{1}{N}\displaystyle\sum_{u,i}^n (\hat{y}_{u,i}-y_{u,i})^{2}} $$

  * **RMSE** is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.

    Lower values of RMSE indicate better fit.
    
    RMSE squares the residual, and is affected by outliers.

    The RMSE is analogous to the standard deviation (MSE to variance) and is a measure of how large the residuals are spread out.


* We define $y_{u,i}$ as the rating for movie i by user u and denote our prediction with $\hat{y}_{u,i}$. The RMSE is then defined as; 

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i}^n (\hat{y}_{u,i}-y_{u,i})^{2}} $$

$$ RMSE = \sqrt{MSE} $$

  * N - the number of user/movie combinations and the sum occurring over all of these combinations.

  * If the RMSE is larger than 1, it means our typical error is larger than one star.

  * The function that computes the RMSE for vectors of ratings and their corresponding predictors;  
  
```{r RMSE function}
RMSE <- function(true_ratings, predicted_ratings){
        sqrt(mean((true_ratings - predicted_ratings)^2))
} 
```

## Naive Model 

### The model

The naive model is the simplest possible recommendation system that predicts the same rating for all movies regardless of the user preferences.

The naive model takes into account a similar rating for all movies and users, and the differences are explained by random variation which will be defined by:

$$ Y_{u, i} = \mu + \epsilon_{u, i} $$

$\epsilon_{u,i}$ ; independent errors sampled from the same distribution centered at 0.

$\mu$ ; true rating for all movies

The estimate that minimizes the RMSE is the least squares estimate of $\mu$ and which is the $\hat{mu}$ : the average rating of all movies across all users

```{r mu hat}
mu_hat <- mean(train_edx$rating)
mu_hat
```

### The model results
  
if we predict all unknown ratings with $\mu$ we obtain the following RMSE 
```{r naive_rmse}
naive_rmse <- RMSE(test_edx$rating, mu_hat)
naive_mse <- MSE(test_edx$rating, mu_hat)

#creating results table
naive_model_results <- tibble(method = "Average only",
                              MSE=naive_mse, RMSE = naive_rmse)
naive_model_results %>% knitr::kable()
```

The RMSE we got is 1.06, which means our typical error is larger than one star, which is not good enough! 

To remind ourselves, the goal is to aspire for RMSE < 0.8649 

## userId 

### User Activity
```{r Users Ratings summarize, echo=FALSE}
train_edx %>%  
  group_by(userId) %>% 
  summarize(count=n()) %>% 
  summarize(mean=round(mean(count)), 
            median=median(count), 
            mode=Mode(count,na.rm=FALSE), 
            min=min(count), max=max(count)) %>%
  knitr::kable()
```
Each of the 70K different usesrs rated in average 103 different movies. The most frequent value of ratings is only 17 movies, but there are 3% users who rated more than 500 movies! 

The distribution is positively skewed, as the mean is larger than the mode and median.

Both plots demonstrates extreme observations (outliers) e.g. very low or high number of ratings, which indicates that some users are more active than others at rating movies.

Outliers affect RMSE which is highly sensitive to them, and therefore we will have to treat this and set penalty term going forward.

```{r figure 4, echo=FALSE}
# graph number of rating dist. by number of users 
train_edx %>% 
  group_by(userId) %>%
  summarize(count=n()) %>% 
  ggplot(aes(count)) +
  geom_histogram(bins = 30, color = "gray20")+ 
  geom_vline(aes(xintercept=median(count),color="median"), 
                 linetype="dashed", size=0.5)+
  geom_vline(aes(xintercept=Mode(count, na.rm = FALSE),color="Mode"), 
                 linetype="dashed", size=0.5)+
  geom_vline(aes(xintercept=mean(count),color="mean"), 
                 linetype="dashed", size=0.5)+
  scale_color_manual(name = "Statistics", 
                     values = c(median = "blue", mean = "red",Mode="green"))+
  scale_x_log10()+
  ggtitle("User Distribution")+ 
  labs(x="Number of Ratings Count", y="Number of Users") 
```

```{r figure 5, echo=FALSE}
# graph number of ratings per user, see extreme observation  
train_edx %>% 
  group_by(userId) %>%
  summarize(count=n()) %>% 
  ggplot(aes(userId,count)) +
  geom_point(alpha=0.1)+
  geom_hline(aes(yintercept=mean(count)),color="red", 
                 linetype="dashed", size=0.5)+
  geom_hline(aes(yintercept=Mode(count, na.rm = FALSE)),color="blue",
                 linetype="dashed", size=0.5)+
  geom_hline(aes(yintercept=median(count)),color="green", 
                 linetype="dashed", size=0.5)+
  ggtitle("Total Number Of Ratings Per user")+ 
  labs(x="userId - 69,878 Unique Users", y="Total Ratings Per User")

```
50% of the ratings are between 3 to 4 stars. Some users love every movie they watch and simply rate most of them 5 stars, but the left tail inidicate users that are very critic and rate lot of 1-2 stars. 

```{r figure 6 user rating score dist, echo=FALSE}
 
train_edx %>% 
  group_by(userId) %>% 
  summarise(rating_score=mean(rating)) %>% 
  ggplot(aes(rating_score))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(train_edx$rating)),color="red",
                 linetype="dashed", size=0.5)+
  ggtitle("Rating score dist. by number of users")+ 
  labs(x="Rating score", y="number of users")
```

### Model 1 - Modeling User Effect

The previous analysis showed that users are rating different from each other. 
also, some are very active while others rarely active.  

We can augment the naive model by adding the term $b_{u}$ to represent average ranking by user u:
$$ Y_{u, i} = \mu + \ b_{u} + \epsilon_{u, i} $$

$b_{u}$ - the user specific effect/"bias" ; average rating by user u regardless of movie i

we can compute the predicted ratings using the next; 
$$\hat{b_{u}}= mean(y_{u, i}-\hat{mu})$$ 
since the least squares estimate $b_{u}$ is just the average of $y_{u, i}-\hat{mu}$ for each user u.

```{r mu}
mu <- mean(train_edx$rating)
```
  * Fit the model
```{r b_u}
fit_user_ave <- 
  train_edx %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - mu))
```
  * Predict the ratings - how much our prediction improves once using y=mu+b_u
```{r user effect predicted ratings}
predicted_ratings <- 
  test_edx %>% 
  left_join(fit_user_ave, by='userId') %>% 
  mutate(predicted=mu+b_u) %>%
  pull(predicted)
```
We can see that the b_u distribution spans a wide symmetric range around 0, which indicates that the users rate equaly for good and bad.
```{r figure 7-8 , echo=FALSE}

# plot the user spesific effect - these estimates very substantially
user_b_u <- 
  fit_user_ave %>%
  ggplot(aes(b_u))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mu),color="red", linetype="dashed", size=0.5)+
  ggtitle("b_u distribution")+
  labs(x="b_u", y="number of users")

# plot the user predicred rating
user_predicted_ratings <-
  test_edx %>% 
  left_join(fit_user_ave, by='userId') %>% 
  mutate(predicted=mu+b_u)%>% 
  group_by(userId) %>% 
  summarise(predicted=mean(predicted)) %>% 
  ggplot(aes(predicted))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mu),color="red", linetype="dashed", size=0.5)+
  ggtitle("Rating score dist.")+ 
  labs(x="Predicted Rating score", y="number of users")

# display 2 plots together
grid.arrange(user_b_u,user_predicted_ratings, ncol=2)
```
  * Model results
```{r model results user effect}
# model RMSE
model_1_rmse <- RMSE(true_ratings=test_edx$rating,
                     predicted_ratings=predicted_ratings)
# model MSE
model_1_mse <- MSE(test_edx$rating,predicted_ratings)

#add the results to the table 
model_1_results <- tibble(method = "User Effect",
                          MSE=model_1_mse, RMSE = model_1_rmse)
model_1_results %>% knitr::kable() 
```
we obtain RMSE = 0.9791, only 8% improvement from the naive model. 

  * The model mean squared errors
```{r figure 9 plot the user mse, echo=FALSE}

test_edx %>% 
  left_join(fit_user_ave, by='userId') %>% 
  select(userId,rating,b_u, title) %>% 
  mutate(predicted=b_u+mu,
         se=((rating-predicted)^2)) %>%
  group_by(userId) %>% 
  summarise(mse=mean(se)) %>% 
  ggplot(aes(mse))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(mse)),color="red", linetype="dashed", size=0.5)+
  ggtitle("User Effect model squared errors")+ 
  labs(x="Mean squared errors", y="number of users")
```
  * Model 1 squared error summarize; 
```{r se user, echo=FALSE}
test_edx %>% 
  left_join(fit_user_ave, by='userId') %>% 
  select(userId,rating,b_u) %>% 
  mutate(predicted=b_u+mu,
         squared_errors=(rating-predicted)^2) %>% 
  select(squared_errors) %>% 
  summary()
``` 
The large distribution of the errors in the plot, with the heavy right tail, and MSE closer to 1 indicate low accuracy. 

### Model 1.1 - Modeling Reg. User Effect

  **Regularization**
  
As mentioned before, RMSE is sensetive to outliers. 

outliers can be interuped as b's "mistakes" -

  * Users that systematically rate only 1 stars or only 5 stars 
  
  * movies that have only one rating and scored 5 stars will apear as "best" movie, the same for "worst" (1 rating of 1 star). 

The supposed "best" and "worst" movies were rated by very few users, in most cases just 1. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of b's, negative or positive, are more likely.

These are noisy estimates that we should not trust, especially when it comes to prediction.

Large errors can increase our RMSE, so we would rather be conservative when unsure.

Regularization permits us to penalize large estimates that are formed using small sample sizes.

The general idea is to add a penalty for large values of b's to the sum of squars equation that we minimize, since having many large b's makes it harder to minimize the equation that were trying to minimize. 

We control the total variability of the user effects specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:

$$ {\frac{1}{N}\displaystyle\sum_{u,i} ({y}_{u,i}-{\mu}-b_{u,i})^{2}}+{\lambda}\displaystyle\sum_{u,i}{b_{u,i}^{2}} $$

The first term is just least squares and the second is a penalty that gets larger when many b's are large. Using calculus we can actually show that the values of bi that minimize this
equation are: 

$$\hat{b_{i}}({\lambda})= \frac{1}{\lambda+n_{i}}\displaystyle\sum_{u} ({y}_{u,i}-{\hat{mu}})$$

where $n_{i}$ is the number of ratings made for movie i. 

This approach will have our desired effect: when our sample size $n_{i}$ is very large, a case which will give us a stable estimate, then the penalty is effectively ignored since . 

However, when the $n_{i}$ is small, then the estimate is shrunken towards 0. The larger, the more we shrink.

lambda is a tuning parameter. we will use cross-validation to choose it. 
we'll create additional partition of the training for cross validation, pick the lambda and evaluate the performance on the previos train+test. 

```{r cv partition}
set.seed(2020, sample.kind="Rounding") 
# if using R 3.5 or earlier, use `set.seed(2020)`

# randomly splitting train set into 90% training set and 10% testing set 
test_index_cv <- 
  createDataPartition(y = train_edx$rating, 
                      times = 1, p = 0.1, list = FALSE)

train_edx_cv <- train_edx %>% slice(-test_index_cv)
temp <- train_edx %>% slice(test_index_cv)

# making sure that the test set includes users and movies that appear in the training set.

test_edx_cv <- 
  temp %>% 
  semi_join(train_edx_cv, by = "movieId") %>%
  semi_join(train_edx_cv, by = "userId")

removed <- anti_join(temp, test_edx_cv)
train_edx_cv <- rbind(train_edx_cv, removed)
rm(temp, removed)
```
  * The model will look like this:

$$ Y_{u, i} = \mu + \ {\lambda}b_{u} + \epsilon_{u, i} $$
${\lambda}$ = Penalty term

  * Choosing penalty term (lambda) for user effect;
```{r lambda for user effect}
equation_mu <- mean(train_edx_cv$rating)

equation_sum_u <- 
  train_edx_cv %>%
  group_by(userId) %>%
  summarize(n_i=n(), 
            s=sum(rating-equation_mu))

lambdas <- seq(0,7,0.05)

user_rmses <- 
  sapply(lambdas,function(lambda){
    reg_predicted_ratings <- 
      test_edx_cv %>%
      left_join(equation_sum_u, by="userId") %>%
      mutate(reg_b_u=(s/(n_i+lambda)),
             predicted=(equation_mu+reg_b_u)) %>%
      pull(predicted)
    
    return(RMSE(true_ratings=test_edx_cv$rating,
                predicted_ratings=reg_predicted_ratings))
  })

qplot(lambdas,user_rmses)
```

```{r user effect penalty term}
penalty_term <- lambdas[which.min(user_rmses)]
penalty_lambda_rmse <- c(lambda=penalty_term,
                         cv_rmse=user_rmses[lambda=penalty_term])
penalty_lambda_rmse
```
  * Apply lambda on edx train and test set
```{r fir reg user effect}
fit_reg_user_ave <- 
  train_edx %>% 
  group_by(userId) %>% 
  summarize(n_i=n(), 
            reg_b_u=(sum(rating - mu)/(n_i+penalty_term)))
```
  * Penalized prediction
```{r reg predicted rating user}
reg_predicted_ratings <- 
  test_edx %>% 
  left_join(fit_reg_user_ave, by='userId') %>%
  mutate(predicted=mu+reg_b_u) %>%
  pull(predicted)
```
  * Penalized model results 
```{r reg user effect results}
model_1_1_rmse <- RMSE(true_ratings=test_edx$rating,
                       predicted_ratings=reg_predicted_ratings)
model_1_1_mse <- MSE(test_edx$rating,reg_predicted_ratings)

# add the results to the table 
model_1_1_results <- tibble(method = "Reg. User Effect",
                            MSE=model_1_1_mse, RMSE = model_1_1_rmse)
model_1_1_results %>% knitr::kable()
```
  * Model 1.1 squared error summarize 
```{r reg se user}
test_edx %>% 
  left_join(fit_reg_user_ave, by='userId') %>% 
  select(userId,rating,reg_b_u) %>% 
  mutate(predicted=reg_b_u+mu,
         reg_squared_errors=(rating-predicted)^2) %>% 
  select(reg_squared_errors) %>% 
  summary()
```
We obtain RMSE= 0.9785479, the penalized estimates does not provide much improvement to the RMSE. 

moving forward by testing the movie specific effect on the predicrion.

## movieId

### Movie Rating
```{r movie summarize, echo=FALSE}
train_edx %>% 
  group_by(movieId) %>%
  summarize(count=n()) %>% 
  summarize(mean=round(mean(count)), 
            median=median(count), 
            mode=Mode(count,na.rm=FALSE), 
            min=min(count), max=max(count)) %>%
  knitr::kable()
```
In average, each one of the 10.6k different movies get rated 674 times.

The movies most frequently gets only 1 ratings, but there are exeptional like "Pulp Fiction" that got rated 3734% (!) more than average with 25169 users ratings.

the distribution is positively skewed, like the user activity dist., as the mean is larger than the mode and median.
```{r figure 10 graph number of rating dist. by number of movies, echo=FALSE}
# graph number of rating dist. by number of movies 
train_edx %>% 
  group_by(movieId) %>%
  summarize(count=n()) %>%
  ggplot(aes(count)) +
  geom_histogram(bins = 30, color = "gray20")+ 
  geom_vline(aes(xintercept=median(count),color="median"), 
                 linetype="dashed", size=0.5)+
  geom_vline(aes(xintercept=Mode(count, na.rm = FALSE),color="Mode"),
                 linetype="dashed", size=0.5)+
  geom_vline(aes(xintercept=mean(count),color="mean"), 
                 linetype="dashed", size=0.5)+
  scale_color_manual(name = "Statistics", 
                     values = c(median = "blue", mean = "red",Mode="green"))+
  scale_x_log10()+
  ggtitle("movieId Distribution")+ 
  labs(x="Number Of Ratings Count", y="Number of Movies") 
```

```{r figure 11 graph number of ratings per movie, echo=FALSE}
# graph number of ratings per movie, see extreme observation
train_edx %>% 
  group_by(title) %>%
  summarize(count=n()) %>%
  ggplot(aes(title,count)) +
  geom_point(alpha=0.1)+
  geom_hline(aes(yintercept=mean(count)),color="red", 
                 linetype="dashed", size=0.5)+
  geom_hline(aes(yintercept=Mode(count, na.rm = FALSE)),
                 color="green", linetype="dashed", size=0.5)+
  geom_hline(aes(yintercept=median(count)),color="blue", 
                 linetype="dashed", size=0.5)+
  ggtitle("Total N. Of Ratings Per Movie")+ 
  labs(x="10,677 Unique Movies", y="Total Ratings Per Movie") + 
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
```

```{r movie rating dist quantiles, include=FALSE}
train_edx %>% 
  group_by(movieId) %>% 
  summarise(rating_score=mean(rating)) %>% summary(rating_score)
```
Some movies are just generally rated higher than others. 

50% of the ratints are between 2.8 to 3.6 stars. very few gets prefect 5 srars,
respectively very few got the shady 1 star. 
```{r figure 12 movie rating dist, echo=FALSE}
train_edx %>% 
  group_by(movieId) %>% 
  summarise(rating_score=mean(rating)) %>% 
  ggplot(aes(rating_score))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mu),color="red", linetype="dashed", size=0.5)+
  ggtitle("Rating score dist. by number of movies")+ 
  labs(x="Rating score", y="number of movies")
```

### Model 2 - Modeling Movie Effect

We can augment the naive model by adding the term $b_{i}$ to represent average ranking for movie i:

$$ Y_{u, i} = \mu + \ b_{i} + \epsilon_{u, i} $$

$b_{i}$ - the movie specific effect/"bias" ;  average rating for movie i regardless of user

we can compute the predicted ratings using the next; 

$$\hat{b_{i}}= mean(y_{u, i}-\hat{mu})$$ 

since the least squares estimate $b_{i}$ is just the average of $y_{u, i}-\hat{mu}$ for each movie i.
  
  * Fit the model
```{r fit movie ave}
fit_movie_ave <- 
  train_edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```
  * Predict the rarings - how much our prediction improves once using y=mu+b_i
```{r movie predicted ratings}
predicted_ratings <- 
  test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>%
  mutate(predicted = mu + b_i) %>% 
  pull(predicted)
```
We can see that the b_i distribution more negatively which means moives get lower ratings.
```{r figure 13-14, echo=FALSE}
# plot the movie spesific effect - these estimates very substantially
movie_b_i <-
  fit_movie_ave %>%
  ggplot(aes(b_i))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mu),color="red", linetype="dashed", size=0.5)+
  ggtitle("b_i distribution")+
  labs(x="b_i", y="number of movies")

# plot the movie predicted rating
movie_predicted_ratings <-
  test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>% 
  mutate(predicted=mu+b_i)%>% 
  group_by(movieId) %>% 
  summarise(predicted=mean(predicted)) %>% 
  ggplot(aes(predicted))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mu),color="red", linetype="dashed", size=0.5)+
  ggtitle("Rating score dist. by n. of movies")+ 
  labs(x="predicted Rating score", y="number of movies")

# plot 2 graphs together
grid.arrange(movie_b_i ,movie_predicted_ratings, ncol=2)
```
  * Model results
```{r movie effect model results}
# model RMSE
model_2_rmse <- RMSE(true_ratings=test_edx$rating,
                     predicted_ratings=predicted_ratings)
# model MSE
model_2_mse <- MSE(test_edx$rating,predicted_ratings)

#add the results to the table 
model_2_results <- tibble(method = "Movie Effect",
                          MSE=model_2_mse, RMSE = model_2_rmse)
model_2_results %>% knitr::kable()
```
We obtain RMSE=0.9439868, improvement of 11% from the naive model RMSE, but it's better than the model with just the user effect.

  * The model mean squared errors
```{r figure 15 plot movie mse, echo=FALSE}
test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>% 
  select(movieId,rating,b_i, title) %>% 
  mutate(predicted=b_i+mu,
         se=((rating-predicted)^2)) %>%
  group_by(movieId) %>% 
  summarise(mse=mean(se)) %>% 
  ggplot(aes(mse))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(mse)),color="red", linetype="dashed", size=0.5)+
  ggtitle("User Effect model squared errors")+ 
  labs(x="Mean squared errors", y="number of movies")
```
  * The model mean squared errors summary
```{r se movie summary}
test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>% 
  select(movieId,rating,b_i) %>% 
  mutate(predicted=b_i+mu,
         squared_errors=(rating-predicted)^2) %>% 
  select(squared_errors) %>% 
  summary()
```
The squared errors distribution is large, which indicate low accuracy. The MSE distribution tail quite shorter than the user effect model, but can be improved by penalize the outliers. 

### Model 2.1 - Modeling Reg. Movie Effect
  
  * The model will look like this:

$$ Y_{u, i} = \mu + \ {\lambda}b_{i} + \epsilon_{u, i} $$
${\lambda}$ = Penalty term

  
  * Choosing penalty term (lambda) for movie effect

```{r lambda for movie effect, echo=TRUE}
equation_mu <- mean(train_edx_cv$rating)

equation_sum_m <- 
  train_edx_cv %>%
  group_by(movieId) %>%
  summarize(n_i=n(), 
            s=sum(rating-equation_mu))

lambdas <- seq(0,6,0.05)

movie_rmses <- 
  sapply(lambdas,function(lambda){
    reg_predicted_ratings <- 
      test_edx_cv %>%
      left_join(equation_sum_m, by="movieId") %>%
      mutate(reg_b_i=(s/(n_i+lambda)),
             predicted=(equation_mu+reg_b_i)) %>%
      pull(predicted)
    return(RMSE(true_ratings=test_edx_cv$rating,
                predicted_ratings=reg_predicted_ratings))
  })

qplot(lambdas,movie_rmses)
```

```{r movie effect penalty term}
penalty_term <- lambdas[which.min(movie_rmses)]
penalty_lambda_rmse <- c(lambda=penalty_term,
                         cv_rmse=movie_rmses[lambda=penalty_term])
penalty_lambda_rmse
```
  * Apply lambda on edx train and test set
```{r fit reg movie ave}
fit_reg_movie_ave <- 
  train_edx %>% 
  group_by(movieId) %>% 
  summarize(n_i=n(), 
            reg_b_i=(sum(rating - mu)/(n_i+penalty_term)))
```
  * Penalized prediction
```{r reg predicted movie ratings}
reg_predicted_ratings <- 
  test_edx %>% 
  left_join(fit_reg_movie_ave, by='movieId') %>%
  mutate(predicted=mu+reg_b_i) %>%
  pull(predicted)
```
  * Penalize model results
```{r reg movie model result}
model_2_1_rmse <- RMSE(true_ratings=test_edx$rating,
                       predicted_ratings=reg_predicted_ratings)

model_2_1_mse <- MSE(test_edx$rating,reg_predicted_ratings)

#add the results to the table 
model_2_1_results <- tibble(method = "Reg. Movie Effect",
                            MSE=model_2_1_mse, RMSE = model_2_1_rmse)
model_2_1_results %>% knitr::kable()
```
The penalized model obtain RMSE= 0.9439218. There is no improvement with the penalty term. 
```{r se movie}
test_edx %>% 
  left_join(fit_reg_movie_ave, by='movieId') %>% 
  select(movieId,rating,reg_b_i) %>% 
  mutate(predicted=reg_b_i+mu,
         reg_squared_errors=(rating-predicted)^2) %>% 
  select(reg_squared_errors) %>% 
  summary()
```
  * Comparing between the estimates
    
To see how the estimates shrink, we'll plot the regularized estimates vs. least squared estimates 
```{r figure 16 estimated plot}
data_frame(original = fit_movie_ave$b_i, 
           regularlized = fit_reg_movie_ave$reg_b_i, 
           n = fit_reg_movie_ave$n_i) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5) 
```
The size of the circle represent the size of $n_{i}$ (count of ratings). 

When $n_{i}$ is small, the values are shrinking more toward zero. 

## movieId and userId

### Model 3 - Modeling Movie + User effect

Modeling the movie effect and the user effect seperatly did not offer more than 7%-10% of improvement to the naive RMSE. 

Common sense tells that although each one has its own effect, users tend to watch and rate movies for a reason. 

Therefore it is much right to model them together. 

The model will look like this:

$$ Y_{u, i} = \mu + \ b_{i}+b_{u,i} + \epsilon_{u, i} $$

$b_{u,i}$ - average rating for movie i with user specific effect.

In this case, if a cranky user (= negative $b_{u,i}$) rates a great movie (= positive $b_{i}$ ), the effects counter each other and we may be able to correctly predict that this user rated this great movie with 3 stars rather than 5 stars, and that should improve our prediction.

  * Fit the model
```{r fit movie user ave}
fit_user_movie_ave <- 
  train_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_ui = mean(rating - mu - b_i))
```
  * Predict the ratings - how much our prediction improves once using y=mu+b_i+b_ui
```{r movie user predicted ratings}
predicted_ratings <- 
  test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>%
  left_join(fit_user_movie_ave, by='userId') %>%
  mutate(predicted = mu+b_i+b_ui) %>%
  pull(predicted)
```
  * Model results
```{r model 3 results}
# model RMSE
model_3_rmse <- RMSE(true_ratings=test_edx$rating,
                     predicted_ratings=predicted_ratings)
# model MSE
model_3_mse <- MSE(test_edx$rating,predicted_ratings)

#add the results to the table 
model_3_results <- tibble(method = "Movie + User Effect",
                          MSE=model_3_mse, RMSE = model_3_rmse)
model_3_results %>% knitr::kable()
```
The model RMSE= 0.8666408, 18.2% improvement from the naive model, which is already very significant and shows good correlation between user and a movie.  

  * The model mean squared errors 
```{r figure 17 plot movie and user mse, echo=FALSE}

test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>%
  left_join(fit_user_movie_ave, by="userId") %>%
  select(movieId,userId,rating,b_i,b_ui,title) %>% 
  mutate(predicted=mu+b_i+b_ui,
         se=((rating-predicted)^2)) %>%
  group_by(movieId) %>% 
  summarise(mse=mean(se)) %>% 
  ggplot(aes(mse))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(mse)),color="red", linetype="dashed", size=0.5)+
  ggtitle("Movie + User Effect model squared errors")+ 
  labs(x="Mean squared errors", y="number of movies")
```
  * The model squared errors summary 
```{r se movie+user, echo=FALSE}
test_edx %>% 
  left_join(fit_movie_ave, by='movieId') %>%
  left_join(fit_user_movie_ave, by="userId") %>%
  select(movieId,userId,rating,b_i,b_ui) %>% 
  mutate(predicted=mu+b_i+b_ui,
         squared_errors=(rating-predicted)^2) %>% 
   select(squared_errors) %>%
  summary()
```
Althogh we saw RMSE improvement, the squared error shows large distribution. 
We will try to "fix" it by the penalizing, and reduce very large absolute values of ${b_{i}}$ and ${b_{u,i}}$

### Model 3.1 - Modeling Reg. Movie + User effect
  
  * The model will look like this:

$$ Y_{u, i} = \mu + \ {\lambda}(b_{i}+b_{u,i}) + \epsilon_{u, i} $$

${\lambda}$ = Penalty term
  
  * Choosing penalty term (lambda) for movie + user effect
```{r model 3.1 penalty term}
lambdas <- seq(0,10,0.25)

equation_mu <- mean(train_edx_cv$rating)

movie_user_rmses <- 
  sapply(lambdas,function(lambda){
    fit_reg_movie_ave <- 
      train_edx_cv %>% 
      group_by(movieId) %>%
      summarize(n_i=n(),
                s= sum(rating - equation_mu),
                reg_b_i=(s/(n_i+lambda)))
    
    fit_reg_user_movie_ave <- 
      train_edx_cv %>%
      left_join(fit_reg_movie_ave, by='movieId') %>%
      group_by(userId) %>%
      summarize(n_i=n(), 
                s= sum(rating -reg_b_i -equation_mu), 
                reg_b_ui=(s/(n_i+lambda)))
    
    reg_predicted_ratings <- 
      test_edx_cv %>% 
      left_join(fit_reg_movie_ave, by='movieId') %>%   
      left_join(fit_reg_user_movie_ave, by='userId') %>%
      mutate(predicted = equation_mu+reg_b_i+reg_b_ui) %>%
      pull(predicted)
    
    return(RMSE(true_ratings=test_edx_cv$rating,
                predicted_ratings=reg_predicted_ratings))
    
  })

qplot(lambdas,movie_user_rmses)
```

```{r model 3 lambda}
penalty_term <- lambdas[which.min(movie_user_rmses)]
penalty_lambda_rmse <- c(lambde=penalty_term,
                         cv_rmse=movie_user_rmses[lambda=penalty_term])
penalty_lambda_rmse
```
  * Apply lambda on edx train and test set
```{r fet reg movie user ave}
fit_reg_movie_ave <- 
  train_edx %>%
  group_by(movieId) %>%
  summarize(n_i=n(), 
            s= sum(rating - mu),
            reg_b_i=(s/(n_i+penalty_term)))

fit_reg_user_movie_ave <- 
  train_edx %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  group_by(userId) %>% 
  summarize(n_i=n(),
            s= sum(rating -reg_b_i -mu), 
            reg_b_ui=(s/(n_i+penalty_term)))
```
  * Penalized prediction
```{r movie+user efect predicted ratings}
reg_predicted_ratings <- 
  test_edx %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  mutate(predicted = mu+reg_b_i+reg_b_ui) %>%
  pull(predicted)
```
  * Penalized model results 
```{r model 3.1 results}
model_3_1_rmse <- RMSE(true_ratings=test_edx$rating,
                       predicted_ratings=reg_predicted_ratings)

model_3_1_mse <- MSE(test_edx$rating,reg_predicted_ratings)

#add the results to the table 
model_3_1_results <- tibble(method = "Reg. Movie + User Effect",
                            MSE=model_3_1_mse, RMSE = model_3_1_rmse)
model_3_1_results %>% knitr::kable()
```
The penalized model obtain RMSE = 0.8659649 which takes us even closer to our goal.

  * The model reg squared errors summary
```{r reg movie user squared errors summary, echo=FALSE}
test_edx %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  mutate(predicted = mu+reg_b_i+reg_b_ui,
         reg_squared_errors=(rating-predicted)^2) %>% 
  select(reg_squared_errors) %>% 
  summary()
```
  * Comparing between the estimates
    
To see how the estimates shrink, we'll plot the regularized estimates vs. least squre estimates 
```{r figure 18 plot estimate}
data_frame(original = fit_user_movie_ave$b_ui, 
           regularlized = fit_reg_user_movie_ave$reg_b_ui,
           n=fit_reg_user_movie_ave$n_i) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5) 
```
 
## Age of the movie at rating

### How old was the movie during rating
  
Having the joint effect of movie and user is not enough since the users ratings are affected by other factors. 

One factor for exampe, is the age of the movie at rating. There are users that likes the quality of "old times" movies and favors aged movies, regardless of the specific movie. It can be considered as "Old Movies Genre".

We will model the effect of the age of the movie on the user.

We also need to rememmber that a movie can be rated multiple times by different users at different years. 

Age of a movie at rating summary; 
```{r age summarize, echo=FALSE}
train_edx %>% 
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  summarize(mean=mean(age_at_rating), 
            median=median(age_at_rating), 
            mode=Mode(age_at_rating,na.rm=FALSE), 
            min=min(age_at_rating), max=max(age_at_rating)) %>% 
  knitr::kable() 
```
The average age of the movies is 12 years. 

The youngest movies being rated were the ones that rated the same year of release. The oldest is 93 years old. 

Most frequently rated are movies one year after they released. 

The next tile plot shows the average score of the movies at rating vs. their release year. 
The tiled colors represent the rate score; darker tile indicates higher score.

We can see that older movies score higher in average, but the effect is not much significant.  

```{r figur 19-20, echo=FALSE}

# graph -  scoring by age 
scoring_by_age <- 
  train_edx %>% 
  group_by(title,rate_year) %>% 
  summarize(count=n(), 
            Ave_rating_score=mean(rating),
            rate_year=rate_year[1], 
            release_year=release_year[1]) %>%
  ggplot(aes(x=release_year,y=rate_year,fill=Ave_rating_score)) + 
  geom_tile() + 
  coord_fixed(expand = FALSE)+
  scale_fill_continuous_sequential(palette = "Blues")+
  ggtitle(" Movie Rating score - Release Year Vs. Rate Year")+ 
  labs(x="Movie Release Year", y="Movie Rate Year") 

# graph - average scoring by age
ave_scoring_by_age <- 
  train_edx %>% 
  group_by(title,rate_year) %>% 
  summarize(count=n(), 
            Ave_rating_score=mean(rating),
            rate_year=rate_year[1], 
            release_year=release_year[1]) %>% 
  group_by(release_year) %>% #mutate(ave=mean(Ave_rating_score)) %>%
  ggplot(aes(x=release_year,y=as.character('mean'),fill=Ave_rating_score)) + 
  geom_tile() + 
  coord_fixed(expand = FALSE)+
  scale_fill_continuous_sequential(palette = "Blues")+
  ggtitle("mean Movie Rating score")+ 
  labs(x="Movie Release Year", y="mean") 

# plot 2 graphs - place multiple grobs on a page
plot_grid(ave_scoring_by_age, scoring_by_age, ncol=1, align="v")
```
The next table show example of movies with more than total of 1000 ratings, that released in 1984. 

The rate years are 1999-2000, which means the age of the movies at rating is around 3 to 4 years. 

In this random example we can see the change of the average rating score for each movie by its age, and the average rate score of the entire rate year.  

 * 1984 release year example (filter movies with less than 1000 ratings)
```{r 1984 release year example, echo=FALSE}
age_ex._1984 <- 
  train_edx %>% 
  group_by(title, rate_year) %>%
  summarize(count=n(),Ave_rating_score = round(mean(rating),3),
            rate_year=rate_year[1],
            release_year=release_year[1]) %>% 
  filter(release_year=="1984" & count>1000) %>% 
  group_by(rate_year) %>%
  mutate(ave = round(mean(Ave_rating_score),3)) %>% arrange(rate_year)
# display first 10 in a table
age_ex._1984[1:10,] %>% 
  knitr::kable() 
``` 
The next plot represent the averge rate score of the age at rating (for movies with 5k total ratings and up). 

Old movies score higher, but they also rated less times, which need to take into penalize considaration.  
```{r figure 21 age ave rating, echo=FALSE}

train_edx %>% 
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  group_by(age_at_rating) %>%
  summarize(k_count=n()/1000, ave=mean(rating)) %>% filter(k_count>5) %>%
  ggplot(aes(age_at_rating,ave)) +
  geom_line(aes(color = k_count))+
  scale_color_gradient2(low = 'white', mid ='blue' , high = 'red')+
  geom_hline(aes(yintercept=mean(train_edx$rating)),color="blue", linetype="dashed", size=0.5)+
  ggtitle("average Ratings distribution by the age of the movie")+ 
  labs(x="Age of a movie at rating", y="Average rating score")
```
The points at the plot represent different movies. 
There are many more newer movies than older, however there is some evidence for age effect 
```{r figure 22 avidence for age effect, echo=FALSE}

train_edx %>% 
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  group_by(title) %>%
  summarize(count = n(), 
            ave_age_at_rating=mean(age_at_rating),
            ave_rating = mean(rating)) %>%
  ggplot(aes(ave_age_at_rating, ave_rating)) +
  geom_point() + 
  geom_smooth()+
  ggtitle("")+ 
  labs(x="ave age at rating", y="Average Rating Score") 
```
### Classical Hollywood cinema

Classical Hollywood cinema, a genre of age, is a term used in film criticism to describe both a narrative and visual style of filmmaking which became characteristic of American cinema between the 1910s (rapidly after World War I) and the 1960s.

(https://en.wikipedia.org/wiki/Classical_Hollywood_cinema)

There are 1.3k Classical Hollywood cinema movies in our data with average age of 54 years old and average rating of 3.9 stars, 0.4 star more than the general average.   
```{r classical summarize, echo=FALSE}
train_edx %>% 
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0 & release_year %in% "1915":"1960") %>%
  summarize(n_movies=n_distinct(movieId),
            ave_rating=mean(rating),
            ave_age=mean(age_at_rating),
            min_age=min(age_at_rating), 
            max_age=max(age_at_rating)) %>%
  knitr::kable()
```
Classical Hollywood cinema 36-93 years old movies and their average score as shown;
```{r figure 23 graph old movies, echo=FALSE}

train_edx %>% 
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0 & release_year %in% "1915":"1960") %>%
  group_by(age_at_rating) %>%
  summarize(count=n(), 
            ave=mean(rating), 
            rating=rating[1]) %>% 
  ggplot(aes(age_at_rating,ave)) +
  geom_point(aes(color = count))+
  geom_hline(aes(yintercept=3.899027),color="blue", linetype="dashed", size=0.5)+
  scale_color_gradient2(low = 'white', mid ='blue' , high = 'red')+
  ggtitle("Classical Hollywood cinema movies 1915-1960")+ 
  labs(x="Age of a movie at rating", y="average rating score") 
```
### Model 4 - Reg. User and Age of the movie at rating Effect

The model will group the age of the movie at rating regardles of its title since
each movie rated several times over the years. 

Previous analysis on the user effect taught us that the model should be penalized, also, the analysis on the age at rating indicate of large variation between the rating counts and the rating scores.

The model will look like this:
$$ Y_{u, i} = \mu + \ {\lambda}b_{u}+b_{a} + \epsilon_{u, i} $$
$b_{a}$ - average rating of a movie based on his age at rating.

We'll choose penalize model in advance; 

 * Choosing penalty term (lambda) for User and Age effect
```{r model 4 penalty term} 
lambdas <- seq(0,10,0.25)

equation_mu <- mean(train_edx_cv$rating)

user_age_rmses <- 
  sapply(lambdas,function(lambda){
    fit_reg_user_ave <- 
      train_edx_cv %>% 
      mutate(age_at_rating= abs(rate_year-release_year)) %>%
      filter(age_at_rating>=0)%>%
      group_by(userId) %>%
      summarize(n_i=n(),
                s= sum(rating - equation_mu),
                reg_b_u=(s/(n_i+lambda)))
    
    fit_reg_user_age_ave <- 
      train_edx_cv %>%
      mutate(age_at_rating= abs(rate_year-release_year)) %>%
      filter(age_at_rating>=0)%>%
      left_join(fit_reg_user_ave, by='userId') %>%
      group_by(age_at_rating) %>%
      summarize(n_i=n(), 
                s= sum(rating -reg_b_u -equation_mu), 
                reg_b_ua=(s/(n_i+lambda)))
    
    reg_predicted_ratings <- 
      test_edx_cv %>% 
      mutate(age_at_rating= abs(rate_year-release_year)) %>%
      filter(age_at_rating>=0)%>%
      left_join(fit_reg_user_ave, by='userId') %>%   
      left_join(fit_reg_user_age_ave, by='age_at_rating') %>%
      mutate(predicted = equation_mu+reg_b_u+reg_b_ua) %>%
      pull(predicted)
    
    return(RMSE(true_ratings=test_edx_cv$rating,
                predicted_ratings=reg_predicted_ratings))
  })

qplot(lambdas,movie_user_rmses)
```

```{r model 4 lambda}
penalty_term <- lambdas[which.min(user_age_rmses)]
penalty_lambda_rmse <- c(lambda=penalty_term,
                         cv_rmse=user_age_rmses[lambda=penalty_term])
penalty_lambda_rmse
```
  * Apply lambda on edx train and test set
```{r fit age ave}
fit_reg_user_ave <- 
  train_edx %>%
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  group_by(userId) %>%
  summarize(n_i=n(), 
            s= sum(rating - mu),
            reg_b_u=(s/(n_i+penalty_term)))

fit_reg_user_age_ave <- 
  train_edx %>%
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  left_join(fit_reg_user_ave, by='userId') %>%
  group_by(age_at_rating) %>% 
  summarize(n_i=n(),
            s= sum(rating -reg_b_u -mu), 
            reg_b_ua=(s/(n_i+penalty_term)))
```
  * Penalized predicted ratings
```{r age predicted rating}
reg_predicted_ratings <- 
  test_edx %>%
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  left_join(fit_reg_user_ave, by='userId') %>%
  left_join(fit_reg_user_age_ave, by='age_at_rating') %>%
  mutate(predicted = mu+reg_b_u+reg_b_ua) %>%
  pull(predicted)
```
  * Penalized model results 
```{r model 4 results}
model_4_rmse <- RMSE(true_ratings=test_edx$rating,
                     predicted_ratings=reg_predicted_ratings)

model_4_mse <- MSE(test_edx$rating,reg_predicted_ratings)

#add the results to the table 
model_4_results <- 
  tibble(method = "Reg. User + Age of the movie at rating Effect",
          MSE=model_4_mse, RMSE = model_4_rmse)
model_4_results %>% knitr::kable()

```
The penalized model obtain RMSE 0.9712367, improvement of only 8.3% than the naive model and only 1% better than the user effect model. 

  * The residual summary as shown; 
```{r reg user age squared errors summary, echo=FALSE}
test_edx %>%
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  left_join(fit_reg_user_ave, by='userId') %>%
  left_join(fit_reg_user_age_ave, by='age_at_rating') %>%
  mutate(predicted = mu+reg_b_u+reg_b_ua,
         reg_squared_errors=(rating-predicted)^2) %>% 
  select(reg_squared_errors) %>% 
  summary()
```
  * The mean squared residual distribution 
```{r figure 24 user age mse plot, echo=FALSE}
test_edx %>%
  mutate(age_at_rating= abs(rate_year-release_year)) %>%
  filter(age_at_rating>=0)%>%
  left_join(fit_reg_user_ave, by='userId') %>%
  left_join(fit_reg_user_age_ave, by='age_at_rating') %>%
  mutate(predicted = mu+reg_b_u+reg_b_ua,
         se=((rating-predicted)^2)) %>%
  group_by(userId) %>% 
  summarise(mse=mean(se)) %>% 
  ggplot(aes(mse))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(mse)),color="red", linetype="dashed", size=0.5)+
  ggtitle("User + Age at rating model MSE dist. ")+ 
  labs(x="Mean squared errors", y="number of users")
```
The model MSE distribution show long right tail that indicates low accuracy. 

We'll move forward and look for more influencing factors. 

## Time Frame Ranges

Movie release stretched over 93 years, while they first started to be rated after 80 years 
```{r time frame ranges, echo=FALSE}
tibble(Year = c("Release", "rate"),
       First = c(min(train_edx$release_year),min(train_edx$rate_year)), 
       Last = c(max(train_edx$release_year),max(train_edx$rate_year)),
       "Range in years" = Last-First) %>% 
  knitr::kable()
```
### Release year

The number of ratings for each movie against the year the movie came out
(each point represents a different movie);
```{r figur 25 number of rating per movie over time, echo=FALSE, out.width = '80%'}

train_edx %>% 
  group_by(movieId) %>% 
  summarize(count = n(), 
            year = as.character(first(release_year))) %>% 
  ggplot(aes(year, count))+
  geom_boxplot(aes(group = cut_width(year, 0.2)), outlier.alpha = 0.1)+
  geom_hline(aes(yintercept=mean(count)),color="red", 
                 linetype="dashed", size=0.5)+ 
  coord_trans(y = "sqrt") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = rel(0.7)))+
  ggtitle("number of rating for each movie per release year")+ 
  labs(x="Movie Release Year", y="number of ratings per movie") 
```
362 movies that releasd in 1995 got the highest ratings count.

Movies that released between the years 1992-1999 shows above average ratings count while starting in 1993 the number of the movies being rated decreases with year. 

The more recent movie is, the less time users have had to rate it. 

In addition, as the total rating count increases, the total rating score decreases. 

Newer released movies gets more rated, but the amount of the ratings affects the average score and lower it. 
```{r figure 26 average rating score for each release year, echo=FALSE}
train_edx %>% 
  group_by(release_year) %>%
  summarize(k_count=n()/1000,      # count in thousands
            rating_score = mean(rating)) %>%
  ggplot(aes(release_year, rating_score)) +
  geom_point(aes(color=k_count)) + 
  scale_color_gradient2(high = 'black', mid ='gray' ,low = 'white' )+
  geom_smooth()+
  geom_hline(aes(yintercept=mean(edx_year_sanitized$rating)),
             color="red", linetype="dashed", size=0.5)+
  ggtitle("Average Rating Per year")+ 
  labs(x="Release Year", y="Average Rating Score") 
```
The next plot shows post-1993 movies - ratings per year and their average ratings. Each point represent unique movie. 

As the rating per year increases, the average rate score increases, in other words, the more often a movie is rated, the higher its average rating.  
```{r figure 27 post-1993 movies, echo=FALSE}

train_edx %>% 
  filter(release_year >= 1993) %>%
  group_by(movieId) %>%
  summarize(count = n(), 
            years = 2018 - first(release_year),
            title = title[1],
            ave_rating = mean(rating)) %>%
  mutate(rating_per_year = count/years) %>%
  ggplot(aes(rating_per_year, ave_rating)) +
  geom_point() +
  geom_smooth()+
  ggtitle("post-1993 movies ratings per year and their average ratings")+ 
  labs(x="Rating Per Year", y="Average Rating Score") 
```
Here are the top 10 movies with the most ratings per year, along with their average ratings (represent the upper right part of the previous plot)
```{r top 10 movies,echo=FALSE}
train_edx %>% 
  filter(release_year >= 1993) %>%
  group_by(movieId) %>%
  summarize(title = title[1], 
            Total_ratings_count = n(), 
            years = 2018 - first(release_year),
            ave_rating = round(mean(rating),2)) %>%
  mutate(rating_per_year =round( Total_ratings_count/years,2)) %>%
  top_n(10, rating_per_year) %>% select(-movieId) %>%
  arrange(desc(rating_per_year, years)) %>%
  knitr::kable()
```

### Rate year
  
The number of the movies being rated generally decreases every year. 

The trend of the average rating score is not distinct, but both plots indicate that there is some evidence of a time effect on average rating.
```{r figure 28-29 time effect evidance, echo=FALSE, out.width = '80%'}
# graph Number of movies per each rate year

rating_count_per_rate_year <- 
  train_edx %>% 
  group_by(movieId) %>%
  summarize(count = n(), 
            rate_year = as.character(first(rate_year))) %>%
  qplot(rate_year, count, data = ., geom = "boxplot") +
  coord_trans(y = "sqrt") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_hline(aes(yintercept=mean(count)),color="red", linetype="dashed", size=0.5)+
  ggtitle("number of movies Per rate year")+ 
  labs(x="Rate Year", y="Movies count") 

# graph average rating for each week 

score_by_week <- 
  train_edx %>% 
  mutate(week_of_rate = round_date(rate_date, unit = "week")) %>%
  group_by(week_of_rate) %>%
  summarize(rating_score = mean(rating)) %>%
  ggplot(aes(week_of_rate, rating_score)) +
  geom_point() +
  geom_smooth()+
  geom_hline(aes(yintercept=mean(rating_score)),color="red", linetype="dashed", size=0.5)+
  ggtitle("Average Rating Per Week")+ 
  labs(x="Week of rate", y="Average Rating Score") 

# plot 2 graphs - place multiple grobs on a page
grid.arrange(rating_count_per_rate_year,score_by_week, ncol=2)
```

### Model 5 - Reg. Movie and User Effect + Time effect 
  
Time effect along with movie effect and user effect supported by previous analysis. 

We'll create a model that adds a time effect to the regularized movie and user joint model. 

We define d_ui as the day for user's u rating of movie i. 

The model will look like this:

$$ Y_{u, i} = \mu + \ {\lambda}(b_{i}+b_{u,i}) + f(d_{u,i}) + \epsilon_{u, i} $$

${f}$ a smooth function of $d_{u,i}$, since time is continuous

  * Fit the model
```{r fit time model} 
fit_time_ave <-  
  train_edx %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  group_by(week) %>%
  summarize(d_ui=mean(rating-mu-reg_b_i-reg_b_ui))
```
  * Predict the ratings
```{r predict time model}
predicted_ratings <- 
  test_edx %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  mutate(predicted = mu+reg_b_i+reg_b_ui+d_ui) %>%
  pull(predicted)
```
  * Model results
```{r model 5 results}
model_5_rmse <- RMSE(true_ratings=test_edx$rating,
                     predicted_ratings=predicted_ratings)

model_5_mse <- MSE(test_edx$rating,predicted_ratings)

#add the results to the table 
model_5_results <- tibble(method = "Reg. Movie + User Effect + Time effect",
                          MSE=model_5_mse, RMSE = model_5_rmse)
model_5_results %>% knitr::kable()

```
  * The residual summary as shown
```{r time effect model residual summary, echo=FALSE}
test_edx %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  mutate(predicted = mu+reg_b_i+reg_b_ui+d_ui,
         reg_squared_errors=(rating-predicted)^2) %>% 
  select(reg_squared_errors) %>% 
  summary()
```
  * The model mean squared errors distribution  
```{r figure 30 time effect model MSE distribution, echo=FALSE}
test_edx %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  mutate(predicted = mu+reg_b_i+reg_b_ui+d_ui,
         se=((rating-predicted)^2)) %>%
  group_by(movieId) %>% 
  summarise(mse=mean(se)) %>% 
  ggplot(aes(mse))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(mse)),color="red", linetype="dashed", size=0.5)+
  ggtitle("User+Movie+Time effect model MSE dist.")+ 
  labs(x="Mean squared errors", y="number of movies")
```
The model obtain RMSE= 0.8658609, improvement of 18.31501% from the naive RMSE and just 0.01% improvement from the regularized movie and user effect model. 

The time effect is barely significant. 


## Genres
    
### Genres Overview
  
Movies are classified to different genres. Some genres are very popular and contain large amount of movies and some are barely heard of.

In addition, different users tend to favor certain genres over others and target their watching and rating habits toward them. 

The genres column include every genre that apllies to the movie. Some movies fall under several genres as folowing; 
```{r example of the diffrent geners, echo=FALSE}
# example of the diffrent geners
train_edx[20:23,] %>% select (title,genres) 
```
We'll separate the edx train and test sets rows for unique genres.
Movie with more than one genre will split into several rows, each row represents a unique genre. 
```{r }
train_edx_genres <- train_edx %>%  separate_rows(genres, sep="\\|")
test_edx_genres <- test_edx %>%  separate_rows(genres, sep="\\|")
```
The distribution of the genres by their total ratings and total average score as follow; 
```{r figure 31 gener dist, echo=FALSE}

train_edx_genres %>% 
  group_by(genres) %>% 
  filter(genres!="(no genres listed)") %>%
  summarize(count = n(),ave_rating=mean(rating)) %>% 
  arrange(desc(count)) %>%
  mutate(percentage=100*count/sum(count)) %>% 
  ggplot(aes(ave_rating,percentage))+
  geom_point()+
  geom_text(aes(label=genres), size=3,  hjust=0,vjust=0)+
  ggtitle("Distribution of movies for each genre")+ 
  labs(x="average rating score", y="percentage of the rating count")
```
There are 19 uniqe genres. 

Drama genre leads with 17% of total ratings and the highst average score. 

The genres with average score over 3.4 stars and on top 20 quantile of total ratings are the following; 
```{r filter genres, echo=FALSE}
train_edx_genres %>% 
  group_by(genres) %>% 
  filter(genres!="(no genres listed)") %>%
  summarize(count_m = round(n()/1000000,2), #number of ratings in millions
            ave_rating=round(mean(rating),2),
            year=release_year[1]) %>% 
  mutate(percentage=round(100*count_m/sum(count_m)),2) %>% 
  filter(ave_rating>=3.4 & count_m>=quantile(count_m, 0.80)) %>% 
  arrange(desc(percentage)) %>%   
  select(genres,count_m,ave_rating, percentage) %>% 
  knitr::kable()
```
We can see that those 4 genres has changed over the years, but in general their ratings respectivly increases until the mid 90's and their average rate score decreases constantly. 
```{r 'figere 32-33' , echo=FALSE, out.width = '80%'}

# rating over years
genres_rating_over_years <- 
  train_edx_genres %>% 
  group_by(genres,release_year) %>% 
  summarize(count_k=n()/1000,     #number of ratings in thousand
            year=release_year[1]) %>%
  filter(genres %in% c("Drama", "Comedy", "Action","Thriller"),
         release_year>="1960") %>%
  ggplot(aes(x =year, y = count_k)) +
  geom_point(aes(color=genres))+
  geom_smooth()

# score over the years

genres_score_over_years <- 
  train_edx_genres %>% 
  group_by(genres,release_year) %>% 
  summarize(ave=mean(rating),
            year=release_year[1]) %>%
  filter(genres %in% c("Drama", "Comedy", "Action", "Thriller"),
         release_year>="1960") %>%
  ggplot(aes(x =year, y = ave)) +
  geom_point(aes(color=genres)) + 
  geom_smooth()

# plot 2 graphs - place multiple grobs on a page
plot_grid(genres_rating_over_years,genres_score_over_years, ncol=1, align="v")
```

The analysis shows evidance of genre effect. We'll add the effect to our previous model. 

### Model 6 - Reg. Movie + User Effect + Time Effect + Genres Effect 

The model will look like this:

$$ Y_{u, i} = \mu + \ {\lambda}(b_{i}+b_{u,i}) + f(d_{u,i}) + \displaystyle\sum_{k=1}^K{x_{u,i}{\beta_{k}}} +\epsilon_{u, i} $$
${x_{u,i}^k=1}$ if ${g_{u,i}}$ is genre k

We simply define ${b_{g}}$ as the unique genre effect (average rating of the genre) correspondent with movie i

  * Fit the model
```{r fit geners model}
fit_genres_ave <-  
  train_edx_genres %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  group_by(genres) %>%
  summarize(b_g=mean(rating-mu-reg_b_i-reg_b_ui-d_ui))
```
  * Predict the ratings
```{r predict geners model ratings}
predicted_ratings <- 
  test_edx_genres %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  left_join(fit_genres_ave, by='genres') %>% 
  mutate(predicted = mu+reg_b_i+reg_b_ui+d_ui+b_g) %>%
  pull(predicted)
```
  * The model results
```{r model 6 results}
model_6_rmse <- RMSE(true_ratings=test_edx_genres$rating,
                     predicted_ratings=predicted_ratings)

model_6_mse <- MSE(test_edx_genres$rating,predicted_ratings)

#add the results to the table 
model_6_results <- 
  tibble(method = "Reg. Movie + User Effect + Time Effect + Genres Effect",
         MSE=model_6_mse, RMSE = model_6_rmse)
model_6_results %>% knitr::kable()

```
  * The model mean squared error distribution
```{r figure 34 model 6 mse plot, echo=FALSE}

test_edx_genres %>% mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  left_join(fit_genres_ave, by='genres') %>% 
  mutate((predicted = mu+reg_b_i+reg_b_ui+d_ui+b_g),
         se=((rating-predicted)^2)) %>% 
  group_by(movieId) %>% 
  summarise(mse=mean(se)) %>% 
  ggplot(aes(mse))+
  geom_histogram(bins=30, color="black")+
  geom_vline(aes(xintercept=mean(mse)),color="red", linetype="dashed", size=0.5)+
  ggtitle("Model 6 MSE dist.")+ 
  labs(x="Mean squared errors", y="number of movies")
```

We have reached our final goal, and all the features got included in the model which indicates good correlation between them. We'll examine the model on the validation set.

# Test On Validation

Following is a summary of all models and their RMSE, and the most accurate one is the one that took into account all the features: 

Reg. Movie + User Effect + Time Effect + Genres Effect 

$$ Y_{u, i} = \mu + \ {\lambda}(b_{i}+b_{u,i}) + f(d_{u,i}) + \displaystyle\sum_{k=1}^K{x_{u,i}{\beta_{k}}} +\epsilon_{u, i} $$

```{r echo=FALSE}
### Models results table
rbind(naive_model_results,
      model_1_results, 
      model_1_1_results,
      model_2_results, 
      model_2_1_results,
      model_3_results, 
      model_3_1_results,
      model_4_results, 
      model_5_results,
      model_6_results) %>%
  arrange(MSE) %>% knitr::kable()

```
## Data Preparation
```{r test validation}
test_validation <- 
  validation %>%  
  mutate (rate_date = date(as_datetime(timestamp)),
          release_year =as.numeric(str_extract(title,"(?<=\\()(\\d{4})(?=\\))")),
          title= str_remove(as.character(title), "(\\(\\d{4}\\))")) %>% 
  select(-timestamp) %>%
  separate_rows(genres, sep="\\|")
```
## Final Model Examination

  * Fit the model
```{r fit final model}
fit_genre_ave <-  
  train_edx_genres %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  group_by(genres) %>%
  summarize(b_g=mean(rating-mu-reg_b_i-reg_b_ui-d_ui))
```
  * Predict the ratings 
```{r predict ratings validation set }
predicted_ratings <- 
  test_validation %>% 
  mutate(week = round_date(rate_date, unit = "week")) %>%
  left_join(fit_reg_movie_ave, by='movieId') %>%
  left_join(fit_reg_user_movie_ave, by='userId') %>%
  left_join(fit_time_ave, by="week") %>%
  left_join(fit_genres_ave, by='genres') %>% 
  mutate(predicted = mu+reg_b_i+reg_b_ui+d_ui+b_g) %>%
  pull(predicted)
```
  * Final model results
```{r final model results }
model_final_rmse <- RMSE(true_ratings=test_validation$rating,
                         predicted_ratings=predicted_ratings)
model_final_mse <- MSE(test_validation$rating,predicted_ratings)

#add the results to the table 
model_final_results <- 
  tibble(method = "final",
         MSE=model_final_mse, RMSE = model_final_rmse)
model_final_results %>% knitr::kable()

```

# Conclusions

The test validation showed even better results than the results we obtained while training the model, which means that the algorithm can act standalone in the future.

  * Future work: we can further add more personalized info on the user, such as age, sex, origin in order to make the model even more accurate.
  
  * Limitations: the dataset size impacted our ability to try and fit more targeted models such as KNN and Matrix Factorization, simply because our peronal computer couldn't handle the load. From the same reasons, it was also challenging to run statistical queries on our current model. Running these in a cloud computing environment would simplify this process.



